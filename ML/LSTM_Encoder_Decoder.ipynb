{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Laura Kulowski\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os, errno\n",
    "import sys\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#encoder class\n",
    "class lstm_encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers = 1):\n",
    "\n",
    "        super(lstm_encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x_input):\n",
    "\n",
    "        lstm_out, self.hidden = self.lstm(x_input.view(x_input.shape[0], x_input.shape[1], self.input_size))\n",
    "\n",
    "        return lstm_out, self.hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "\n",
    "\n",
    "# decoder class\n",
    "class lstm_decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers = 1):\n",
    "        super(lstm_decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x_input, hidden_state):\n",
    "\n",
    "        lstm_out, self.hidden = self.lstm(x_input, hidden_state)\n",
    "        output = self.fc(lstm_out.squeeze(0))\n",
    "\n",
    "        return output, self.hidden\n",
    "    \n",
    "\n",
    "#full class\n",
    "class lstm_seq2seq(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(lstm_seq2seq, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.encoder = lstm_encoder(input_size, hidden_size)\n",
    "        self.decoder = lstm_decoder(input_size, hidden_size)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
