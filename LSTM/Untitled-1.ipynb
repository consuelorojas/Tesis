{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate sample data (replace this with your actual time series data)\n",
    "# Example: X.shape = (num_samples, num_time_steps, num_features)\n",
    "#          y.shape = (num_samples, output_dim)\n",
    "X = np.random.rand(100, 10, 2)  # Multivariate input (2 features)\n",
    "y = np.random.rand(100, 1)\n",
    "\n",
    "# Define the LSTM model with an added hidden layer\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, activation='relu', return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(LSTM(units=50, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))  # Output layer with sigmoid activation\n",
    "\n",
    "# Compile the model with RMSE loss\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')  # RMSE is used as the loss for regression problems\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Evaluate the model (optional)\n",
    "loss = model.evaluate(X, y)\n",
    "print(f'Root Mean Squared Error on training data: {np.sqrt(loss)}')\n",
    "\n",
    "# You can now use the trained model for predicting future values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sample data (replace this with your actual time series data)\n",
    "# Example: X.shape = (num_samples, num_time_steps, num_features)\n",
    "#          y.shape = (num_samples, output_dim)\n",
    "X = torch.rand((100, 10, 2))  # Multivariate input (2 features)\n",
    "y = torch.rand((100, 2))  # Two output variables\n",
    "\n",
    "# Define the LSTM model with an added hidden layer\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define the Encoder-Decoder LSTM model\n",
    "class EncoderDecoderLSTM(nn.Module):\n",
    "    def __init__(self, n_features, n_steps_in, n_steps_out, hidden_size):\n",
    "        super(EncoderDecoderLSTM, self).__init__()\n",
    "        self.encoder = nn.LSTM(n_features, hidden_size, batch_first=True)\n",
    "        self.repeat_vector = nn.ReplicationPad1d(n_steps_out)\n",
    "        self.decoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, n_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.encoder(x)\n",
    "        x = self.repeat_vector(x)\n",
    "        x, _ = self.decoder(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Set hyperparameters\n",
    "input_size = 2\n",
    "hidden_size = 50\n",
    "output_size = 2  # Two output variables\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = LSTMModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X = torch.Tensor(X)\n",
    "y = torch.Tensor(y)\n",
    "\n",
    "# Lists to store training and test losses\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, X.shape[0] - batch_size + 1, batch_size):\n",
    "        batch_X = X[i:i+batch_size]\n",
    "        batch_y = y[i:i+batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(batch_X)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(predictions, batch_y)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Training loss\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    # Validation (test) loss\n",
    "    with torch.no_grad():\n",
    "        test_predictions = model(X)\n",
    "        test_loss = criterion(test_predictions, y)\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "    # Print the training and test loss for each epoch\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {loss.item()}, Test Loss: {test_loss.item()}')\n",
    "\n",
    "# Plotting the training and test losses\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, epochs + 1), test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Test Losses')\n",
    "plt.show()\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(X)\n",
    "\n",
    "# Evaluate the model (optional)\n",
    "loss = criterion(predictions, y)\n",
    "print(f'Mean Squared Error on training data: {loss.item()}')\n",
    "\n",
    "# You can now use the trained model for predicting future values\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_backbone(num_epoch, model, X, y, Xval, yval, batchsize, dataaugmentation, criterion, optimizer, filepath, avg, n_save):\n",
    "  global device\n",
    "\n",
    "  train_losses = []       # to track the training loss as the model trains\n",
    "  valid_losses = []       # to track the validation loss as the model trains\n",
    "  \n",
    "  avg_train_losses = []   # to track the average training loss per epoch as the model trains\n",
    "  avg_valid_losses = []   # to track the average validation loss per epoch as the model trains\n",
    "\n",
    "  dataloader_val = load_data(Xval, yval, batch_size = batchsize, data_augmentation = False)\n",
    "  dataloader_train= load_data(X, y, batch_size = batchsize, data_augmentation = False)\n",
    "  #print(dataloader_train.shape)\n",
    "\n",
    "  for epoch in range(num_epoch):\n",
    "      #dataloader_train= load_data(X, y, batch_size = batchsize, data_augmentation = dataaugmentation)\n",
    "    \n",
    "      try:\n",
    "        model.train()\n",
    "\n",
    "        # Train on the current epoch\n",
    "        for i, data in enumerate(dataloader_train, 0):\n",
    "            inputs = data[\"features\"].to(device)\n",
    "            labels = data[\"labels\"].to(device).long()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels).cuda()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        # Compute validation loss and accuracy for current epoch\n",
    "        model.eval()\n",
    "        with torch.no_grad():  \n",
    "          for i, data in enumerate(dataloader_val, 0):\n",
    "              inputs = data[\"features\"].to(device)\n",
    "              labels = data[\"labels\"].to(device).long()\n",
    "\n",
    "              outputs = model(inputs)\n",
    "              # Calcular loss de validación\n",
    "              loss = criterion(outputs, labels).cuda()\n",
    "              # record validation loss\n",
    "              valid_losses.append(loss.item())\n",
    "\n",
    "        # No se debe guardar checkpoints en cada época (guardarlos cada 50 épocas)\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "\n",
    "        # limpiar listas para la proxima epoca:\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "\n",
    "        # imprimir resultados por epoca:  \n",
    "        sys.stdout.write(f\"\\rÉpoca: {epoch} Training Loss: {train_loss} Validation Loss: {valid_loss}\")\n",
    "\n",
    "        stop = earlyStop(train_loss, valid_loss, avg_train_losses, avg_valid_losses, avg)\n",
    "        \n",
    "        if  stop == True:  # caso en que hay que deterner\n",
    "            # cargar checkpoint, o guardar\n",
    "            print(\" - Early stopping\")\n",
    "            break\n",
    "\n",
    "        # condición de checkpoint:\n",
    "        if (epoch % n_save) == 0:\n",
    "          torch.save(model.state_dict(), filepath)     \n",
    "      \n",
    "      # Interrupt Training\n",
    "      except KeyboardInterrupt:\n",
    "        print(\"\\nEntrenamiento interrumpido\")\n",
    "        break\n",
    "\n",
    "  # load the last checkpoint with the best model\n",
    "  model.load_state_dict(torch.load(filepath))\n",
    "\n",
    "  print('\\nEntrenamiento finalizado')\n",
    "  return model, avg_train_losses, avg_valid_losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
